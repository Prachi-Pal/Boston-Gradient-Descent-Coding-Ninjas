{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd642bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the libraries and loading the data\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "train_data = np.loadtxt(\"0000000000002417_training_boston_x_y_train.csv\",delimiter=\",\")\n",
    "test_data = np.loadtxt(\"0000000000002417_test_boston_x_test.csv\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d09d5123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 14)\n",
      "(127, 13)\n"
     ]
    }
   ],
   "source": [
    "#Checking the shape and size of data\n",
    "print(train_data.shape)\n",
    "print(test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31bb0cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 13)\n",
      "(379,)\n",
      "(127, 13)\n"
     ]
    }
   ],
   "source": [
    "#Arranging the data\n",
    "X_train = train_data[:,:-1]\n",
    "Y_train = train_data[:,-1]\n",
    "X_test  = test_data\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15a8c2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Gradient Descent Algorithm to the Data\n",
    "def step_gradient(X,Y,m,alpha):\n",
    "    m_slope=np.zeros(len(X[0]))\n",
    "    for i in range(len(X)):\n",
    "        x_new = X[i]\n",
    "        y_new = Y[i]\n",
    "        for j in range(len(x_new)):\n",
    "            m_slope[j]+=(-2/len(X))*(y_new-sum(m*x_new)) * x_new[j]\n",
    "    new_m=m-(alpha*m_slope)\n",
    "    return new_m\n",
    "    \n",
    "def cost(m,x,y):\n",
    "    total_cost=0\n",
    "    for i in range(len(x)):\n",
    "        total_cost+=(1/len(x))*((y[i]-sum(m*x[i]))**2)\n",
    "    return total_cost\n",
    "\n",
    "def gd(x,y,alpha,repeat):\n",
    "    m=np.zeros(len(x[0]))\n",
    "    for i in range(repeat):\n",
    "        m=step_gradient(x, y, m, alpha)\n",
    "        print(\"Loop Number\", i,\"--> cost =\",cost(m,x,y))\n",
    "    return m     \n",
    "\n",
    "def run(x,y):\n",
    "    repeat=250\n",
    "    alpha=0.0057\n",
    "    x=np.append(x,np.ones(len(x)).reshape(-1,1),axis=1)\n",
    "    m=gd(x,y,alpha,repeat)\n",
    "    return m "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75dde2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.40784991 -0.48772236 -1.2660231  ... -0.18508661  0.49497715\n",
      "  -1.32371742]\n",
      " [-0.40737368 -0.48772236  0.24705682 ... -0.04412552  0.07887577\n",
      "  -0.14099296]\n",
      " [ 0.1251786  -0.48772236  1.01599907 ... 12.8386773  -3.01392315\n",
      "   0.70752871]\n",
      " ...\n",
      " [-0.40831101 -0.48772236  0.24705682 ... -0.03683424  0.03705351\n",
      "  -0.03727409]\n",
      " [-0.41061997 -0.48772236 -1.15221381 ... -0.03076159  0.11272547\n",
      "  -0.41308112]\n",
      " [ 0.34290895 -0.48772236  1.01599907 ... -0.20441415  0.71605641\n",
      "  -2.50832339]]\n"
     ]
    }
   ],
   "source": [
    "# Feature Scaling \n",
    "features = []\n",
    "for i in range(len(X_train[0])):\n",
    "    for j in range(i,len(X_train[0])):\n",
    "        for k in range(j,len(X_train[0])):\n",
    "            features.append(X_train[:,i]*X_train[:,j]*X_train[:,k])\n",
    "features = np.array(features)\n",
    "for i in features:\n",
    "    X_train=np.append(X_train,i.reshape(-1,1),axis=1)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e648350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming the Data\n",
    "scaler=preprocessing.StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_new = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f08818ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.40098068, -0.49042688, -1.28149216, ..., -0.27411059,\n",
       "         0.26307663, -0.50901962],\n",
       "       [-0.40053396, -0.49042688,  0.20753471, ..., -0.24302857,\n",
       "         0.13769178, -0.23637179],\n",
       "       [ 0.09900799, -0.49042688,  0.96425328, ...,  2.5976381 ,\n",
       "        -0.7942689 , -0.04076614],\n",
       "       ...,\n",
       "       [-0.40141319, -0.49042688,  0.20753471, ..., -0.24142084,\n",
       "         0.12508938, -0.21246198],\n",
       "       [-0.40357903, -0.49042688, -1.16949207, ..., -0.24008182,\n",
       "         0.1478918 , -0.29909498],\n",
       "       [ 0.30324229, -0.49042688,  0.96425328, ..., -0.27837232,\n",
       "         0.32969499, -0.78210118]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new #Checking the new values of X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "230b5343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Number 0 --> cost = 557.5690924192342\n",
      "Loop Number 1 --> cost = 534.3137255525065\n",
      "Loop Number 2 --> cost = 517.2106192501158\n",
      "Loop Number 3 --> cost = 502.6100707827003\n",
      "Loop Number 4 --> cost = 489.3155037170919\n",
      "Loop Number 5 --> cost = 476.87121216141884\n",
      "Loop Number 6 --> cost = 465.06498322815173\n",
      "Loop Number 7 --> cost = 453.77670449895766\n",
      "Loop Number 8 --> cost = 442.9281988588369\n",
      "Loop Number 9 --> cost = 432.4641692779497\n",
      "Loop Number 10 --> cost = 422.34356544716326\n",
      "Loop Number 11 --> cost = 412.5349420448478\n",
      "Loop Number 12 --> cost = 403.0136261775936\n",
      "Loop Number 13 --> cost = 393.75984896151203\n",
      "Loop Number 14 --> cost = 384.75745695046834\n",
      "Loop Number 15 --> cost = 375.9929998112567\n",
      "Loop Number 16 --> cost = 367.45507326518515\n",
      "Loop Number 17 --> cost = 359.13383992187164\n",
      "Loop Number 18 --> cost = 351.0206762774348\n",
      "Loop Number 19 --> cost = 343.1079103378355\n",
      "Loop Number 20 --> cost = 335.38862500331777\n",
      "Loop Number 21 --> cost = 327.85650958614906\n",
      "Loop Number 22 --> cost = 320.50574683148125\n",
      "Loop Number 23 --> cost = 313.33092631050584\n",
      "Loop Number 24 --> cost = 306.32697753247527\n",
      "Loop Number 25 --> cost = 299.4891178925332\n",
      "Loop Number 26 --> cost = 292.8128118478607\n",
      "Loop Number 27 --> cost = 286.2937386406263\n",
      "Loop Number 28 --> cost = 279.9277665630067\n",
      "Loop Number 29 --> cost = 273.71093225738997\n",
      "Loop Number 30 --> cost = 267.6394239132357\n",
      "Loop Number 31 --> cost = 261.7095674961448\n",
      "Loop Number 32 --> cost = 255.9178153497041\n",
      "Loop Number 33 --> cost = 250.26073666473616\n",
      "Loop Number 34 --> cost = 244.7350094269511\n",
      "Loop Number 35 --> cost = 239.337413542227\n",
      "Loop Number 36 --> cost = 234.06482490596628\n",
      "Loop Number 37 --> cost = 228.9142102343695\n",
      "Loop Number 38 --> cost = 223.88262251493023\n",
      "Loop Number 39 --> cost = 218.96719696386137\n",
      "Loop Number 40 --> cost = 214.1651474016856\n",
      "Loop Number 41 --> cost = 209.4737629764834\n",
      "Loop Number 42 --> cost = 204.89040517852015\n",
      "Loop Number 43 --> cost = 200.41250510110498\n",
      "Loop Number 44 --> cost = 196.03756091126\n",
      "Loop Number 45 --> cost = 191.76313550067206\n",
      "Loop Number 46 --> cost = 187.58685429283236\n",
      "Loop Number 47 --> cost = 183.50640318659677\n",
      "Loop Number 48 --> cost = 179.51952661984606\n",
      "Loop Number 49 --> cost = 175.6240257396801\n",
      "Loop Number 50 --> cost = 171.8177566678027\n",
      "Loop Number 51 --> cost = 168.09862885154638\n",
      "Loop Number 52 --> cost = 164.46460349243782\n",
      "Loop Number 53 --> cost = 160.91369204539438\n",
      "Loop Number 54 --> cost = 157.44395478261316\n",
      "Loop Number 55 --> cost = 154.05349941701414\n",
      "Loop Number 56 --> cost = 150.74047978076823\n",
      "Loop Number 57 --> cost = 147.5030945549856\n",
      "Loop Number 58 --> cost = 144.33958604710986\n",
      "Loop Number 59 --> cost = 141.2482390129516\n",
      "Loop Number 60 --> cost = 138.22737952062403\n",
      "Loop Number 61 --> cost = 135.27537385392574\n",
      "Loop Number 62 --> cost = 132.390627452961\n",
      "Loop Number 63 --> cost = 129.57158388998604\n",
      "Loop Number 64 --> cost = 126.81672387866315\n",
      "Loop Number 65 --> cost = 124.12456431505014\n",
      "Loop Number 66 --> cost = 121.49365734879522\n",
      "Loop Number 67 --> cost = 118.92258948313047\n",
      "Loop Number 68 --> cost = 116.40998070235916\n",
      "Loop Number 69 --> cost = 113.95448362562892\n",
      "Loop Number 70 --> cost = 111.55478268587134\n",
      "Loop Number 71 --> cost = 109.20959333285998\n",
      "Loop Number 72 --> cost = 106.91766125940853\n",
      "Loop Number 73 --> cost = 104.67776164979433\n",
      "Loop Number 74 --> cost = 102.48869844954797\n",
      "Loop Number 75 --> cost = 100.34930365579872\n",
      "Loop Number 76 --> cost = 98.25843662741453\n",
      "Loop Number 77 --> cost = 96.21498341421785\n",
      "Loop Number 78 --> cost = 94.21785610459428\n",
      "Loop Number 79 --> cost = 92.26599219085313\n",
      "Loop Number 80 --> cost = 90.35835395172512\n",
      "Loop Number 81 --> cost = 88.49392785141941\n",
      "Loop Number 82 --> cost = 86.67172395468519\n",
      "Loop Number 83 --> cost = 84.8907753573543\n",
      "Loop Number 84 --> cost = 83.15013763186208\n",
      "Loop Number 85 --> cost = 81.44888828726795\n",
      "Loop Number 86 --> cost = 79.78612624332062\n",
      "Loop Number 87 --> cost = 78.16097131812758\n",
      "Loop Number 88 --> cost = 76.57256372901287\n",
      "Loop Number 89 --> cost = 75.02006360616211\n",
      "Loop Number 90 --> cost = 73.50265051866879\n",
      "Loop Number 91 --> cost = 72.01952301261448\n",
      "Loop Number 92 --> cost = 70.56989816082822\n",
      "Loop Number 93 --> cost = 69.153011123984\n",
      "Loop Number 94 --> cost = 67.76811472270977\n",
      "Loop Number 95 --> cost = 66.41447902039184\n",
      "Loop Number 96 --> cost = 65.09139091637189\n",
      "Loop Number 97 --> cost = 63.79815374924363\n",
      "Loop Number 98 --> cost = 62.53408690996732\n",
      "Loop Number 99 --> cost = 61.298525464529504\n",
      "Loop Number 100 --> cost = 60.09081978588667\n",
      "Loop Number 101 --> cost = 58.910335194937254\n",
      "Loop Number 102 --> cost = 57.75645161027856\n",
      "Loop Number 103 --> cost = 56.628563206511274\n",
      "Loop Number 104 --> cost = 55.52607808086228\n",
      "Loop Number 105 --> cost = 54.448417927903876\n",
      "Loop Number 106 --> cost = 53.395017722156155\n",
      "Loop Number 107 --> cost = 52.36532540836318\n",
      "Loop Number 108 --> cost = 51.358801599243094\n",
      "Loop Number 109 --> cost = 50.374919280516885\n",
      "Loop Number 110 --> cost = 49.41316352302657\n",
      "Loop Number 111 --> cost = 48.47303120175994\n",
      "Loop Number 112 --> cost = 47.554030721604676\n",
      "Loop Number 113 --> cost = 46.65568174965824\n",
      "Loop Number 114 --> cost = 45.77751495392783\n",
      "Loop Number 115 --> cost = 44.919071748257\n",
      "Loop Number 116 --> cost = 44.07990404332143\n",
      "Loop Number 117 --> cost = 43.25957400354175\n",
      "Loop Number 118 --> cost = 42.45765380976329\n",
      "Loop Number 119 --> cost = 41.67372542755966\n",
      "Loop Number 120 --> cost = 40.90738038101821\n",
      "Loop Number 121 --> cost = 40.15821953187294\n",
      "Loop Number 122 --> cost = 39.42585286385041\n",
      "Loop Number 123 --> cost = 38.70989927210006\n",
      "Loop Number 124 --> cost = 38.00998635758461\n",
      "Loop Number 125 --> cost = 37.32575022630653\n",
      "Loop Number 126 --> cost = 36.65683529325384\n",
      "Loop Number 127 --> cost = 36.00289409094784\n",
      "Loop Number 128 --> cost = 35.36358708248209\n",
      "Loop Number 129 --> cost = 34.73858247894154\n",
      "Loop Number 130 --> cost = 34.12755606109666\n",
      "Loop Number 131 --> cost = 33.530191005267945\n",
      "Loop Number 132 --> cost = 32.94617771325976\n",
      "Loop Number 133 --> cost = 32.37521364626605\n",
      "Loop Number 134 --> cost = 31.817003162650906\n",
      "Loop Number 135 --> cost = 31.271257359511704\n",
      "Loop Number 136 --> cost = 30.737693917932777\n",
      "Loop Number 137 --> cost = 30.21603695184243\n",
      "Loop Number 138 --> cost = 29.706016860385557\n",
      "Loop Number 139 --> cost = 29.207370183728905\n",
      "Loop Number 140 --> cost = 28.719839462215983\n",
      "Loop Number 141 --> cost = 28.24317309879261\n",
      "Loop Number 142 --> cost = 27.777125224624644\n",
      "Loop Number 143 --> cost = 27.321455567832256\n",
      "Loop Number 144 --> cost = 26.8759293252664\n",
      "Loop Number 145 --> cost = 26.440317037255614\n",
      "Loop Number 146 --> cost = 26.01439446525282\n",
      "Loop Number 147 --> cost = 25.597942472313083\n",
      "Loop Number 148 --> cost = 25.19074690633589\n",
      "Loop Number 149 --> cost = 24.792598486006213\n",
      "Loop Number 150 --> cost = 24.40329268937133\n",
      "Loop Number 151 --> cost = 24.0226296449906\n",
      "Loop Number 152 --> cost = 23.650414025598025\n",
      "Loop Number 153 --> cost = 23.28645494421846\n",
      "Loop Number 154 --> cost = 22.930565852680054\n",
      "Loop Number 155 --> cost = 22.582564442466087\n",
      "Loop Number 156 --> cost = 22.24227254785191\n",
      "Loop Number 157 --> cost = 21.90951605127324\n",
      "Loop Number 158 --> cost = 21.58412479087352\n",
      "Loop Number 159 --> cost = 21.26593247017971\n",
      "Loop Number 160 --> cost = 20.95477656985602\n",
      "Loop Number 161 --> cost = 20.650498261488163\n",
      "Loop Number 162 --> cost = 20.352942323349478\n",
      "Loop Number 163 --> cost = 20.061957058103918\n",
      "Loop Number 164 --> cost = 19.777394212399887\n",
      "Loop Number 165 --> cost = 19.49910889831135\n",
      "Loop Number 166 --> cost = 19.226959516582927\n",
      "Loop Number 167 --> cost = 18.96080768163757\n",
      "Loop Number 168 --> cost = 18.700518148304752\n",
      "Loop Number 169 --> cost = 18.445958740230946\n",
      "Loop Number 170 --> cost = 18.197000279931302\n",
      "Loop Number 171 --> cost = 17.953516520446044\n",
      "Loop Number 172 --> cost = 17.71538407856375\n",
      "Loop Number 173 --> cost = 17.482482369574935\n",
      "Loop Number 174 --> cost = 17.25469354352115\n",
      "Loop Number 175 --> cost = 17.031902422904363\n",
      "Loop Number 176 --> cost = 16.813996441823296\n",
      "Loop Number 177 --> cost = 16.60086558650316\n",
      "Loop Number 178 --> cost = 16.392402337187388\n",
      "Loop Number 179 --> cost = 16.188501611358934\n",
      "Loop Number 180 --> cost = 15.989060708261238\n",
      "Loop Number 181 --> cost = 15.793979254688349\n",
      "Loop Number 182 --> cost = 15.603159152015186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Number 183 --> cost = 15.41650452443914\n",
      "Loop Number 184 --> cost = 15.233921668405218\n",
      "Loop Number 185 --> cost = 15.055319003187329\n",
      "Loop Number 186 --> cost = 14.880607022599271\n",
      "Loop Number 187 --> cost = 14.709698247809065\n",
      "Loop Number 188 --> cost = 14.542507181231658\n",
      "Loop Number 189 --> cost = 14.378950261474667\n",
      "Loop Number 190 --> cost = 14.218945819313467\n",
      "Loop Number 191 --> cost = 14.062414034671507\n",
      "Loop Number 192 --> cost = 13.909276894582963\n",
      "Loop Number 193 --> cost = 13.759458152115199\n",
      "Loop Number 194 --> cost = 13.612883286228543\n",
      "Loop Number 195 --> cost = 13.469479462552606\n",
      "Loop Number 196 --> cost = 13.329175495057436\n",
      "Loop Number 197 --> cost = 13.191901808599003\n",
      "Loop Number 198 --> cost = 13.057590402319533\n",
      "Loop Number 199 --> cost = 12.926174813882332\n",
      "Loop Number 200 --> cost = 12.797590084522357\n",
      "Loop Number 201 --> cost = 12.6717727248938\n",
      "Loop Number 202 --> cost = 12.548660681696413\n",
      "Loop Number 203 --> cost = 12.4281933050624\n",
      "Loop Number 204 --> cost = 12.310311316687017\n",
      "Loop Number 205 --> cost = 12.194956778685366\n",
      "Loop Number 206 --> cost = 12.082073063158854\n",
      "Loop Number 207 --> cost = 11.971604822455229\n",
      "Loop Number 208 --> cost = 11.863497960106153\n",
      "Loop Number 209 --> cost = 11.757699602426761\n",
      "Loop Number 210 --> cost = 11.65415807076218\n",
      "Loop Number 211 --> cost = 11.552822854366207\n",
      "Loop Number 212 --> cost = 11.453644583897544\n",
      "Loop Number 213 --> cost = 11.35657500551951\n",
      "Loop Number 214 --> cost = 11.261566955589606\n",
      "Loop Number 215 --> cost = 11.168574335925035\n",
      "Loop Number 216 --> cost = 11.077552089631576\n",
      "Loop Number 217 --> cost = 10.988456177482334\n",
      "Loop Number 218 --> cost = 10.901243554834263\n",
      "Loop Number 219 --> cost = 10.815872149070003\n",
      "Loop Number 220 --> cost = 10.7323008375528\n",
      "Loop Number 221 --> cost = 10.650489426083306\n",
      "Loop Number 222 --> cost = 10.57039862784627\n",
      "Loop Number 223 --> cost = 10.49199004283623\n",
      "Loop Number 224 --> cost = 10.415226137751192\n",
      "Loop Number 225 --> cost = 10.34007022634361\n",
      "Loop Number 226 --> cost = 10.266486450218224\n",
      "Loop Number 227 --> cost = 10.194439760066581\n",
      "Loop Number 228 --> cost = 10.123895897328104\n",
      "Loop Number 229 --> cost = 10.05482137626846\n",
      "Loop Number 230 --> cost = 9.987183466464916\n",
      "Loop Number 231 --> cost = 9.920950175690265\n",
      "Loop Number 232 --> cost = 9.856090233185522\n",
      "Loop Number 233 --> cost = 9.792573073312893\n",
      "Loop Number 234 --> cost = 9.730368819580518\n",
      "Loop Number 235 --> cost = 9.669448269029752\n",
      "Loop Number 236 --> cost = 9.60978287697771\n",
      "Loop Number 237 --> cost = 9.551344742106195\n",
      "Loop Number 238 --> cost = 9.494106591889594\n",
      "Loop Number 239 --> cost = 9.438041768353688\n",
      "Loop Number 240 --> cost = 9.38312421415803\n",
      "Loop Number 241 --> cost = 9.329328458994551\n",
      "Loop Number 242 --> cost = 9.276629606294982\n",
      "Loop Number 243 --> cost = 9.225003320240248\n",
      "Loop Number 244 --> cost = 9.174425813064852\n",
      "Loop Number 245 --> cost = 9.124873832649627\n",
      "Loop Number 246 --> cost = 9.076324650396112\n",
      "Loop Number 247 --> cost = 9.028756049376435\n",
      "Loop Number 248 --> cost = 8.982146312751992\n",
      "Loop Number 249 --> cost = 8.936474212455343\n"
     ]
    }
   ],
   "source": [
    "m_new = run(X_train_new,Y_train) #Calling the function for Gradient Descent Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de0cfeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.29797452962284\n",
      "24.398395226067407\n",
      "18.048877999945912\n",
      "20.45354714679175\n",
      "20.05494281216249\n",
      "5.8048637248386505\n",
      "26.21369068712394\n",
      "23.306102777715502\n",
      "17.506296130812657\n",
      "20.860149707925352\n",
      "21.904409815210908\n",
      "16.14335409247208\n",
      "18.643661227107987\n",
      "20.65438150513003\n",
      "51.12497949361018\n",
      "21.029246504094477\n",
      "22.85327745428445\n",
      "25.406205587119633\n",
      "16.92734899597253\n",
      "29.788455549192363\n",
      "21.08775096079913\n",
      "20.82438778132807\n",
      "35.394736245659765\n",
      "30.836499944825633\n",
      "35.175961716276575\n",
      "18.584023408308944\n",
      "20.293168380985115\n",
      "29.378979990141993\n",
      "20.14796833164841\n",
      "26.300763824844267\n",
      "16.565657569439946\n",
      "23.311389858261418\n",
      "20.189778344226816\n",
      "23.16718312614068\n",
      "10.26220746048792\n",
      "25.63527559987882\n",
      "23.595410089613765\n",
      "18.105831365993293\n",
      "22.02494571819934\n",
      "11.13844193800927\n",
      "5.677144988903489\n",
      "25.70428789826139\n",
      "26.30128534515722\n",
      "19.561012747130047\n",
      "18.570319564513202\n",
      "-0.5623016748823204\n",
      "46.95663402645522\n",
      "22.386474304370278\n",
      "34.42477354103707\n",
      "10.625448984354563\n",
      "15.735049591635182\n",
      "44.376373714433385\n",
      "11.26624563378642\n",
      "19.71191516314577\n",
      "14.529450724585205\n",
      "19.751642029088895\n",
      "18.93306328552201\n",
      "21.053349236888312\n",
      "13.692678722527567\n",
      "12.693964973852326\n",
      "7.084509528708288\n",
      "23.845861788180525\n",
      "23.352096561167645\n",
      "22.54869619065254\n",
      "15.763529429948653\n",
      "10.421527926698536\n",
      "31.961215268770882\n",
      "13.46577874793539\n",
      "22.272360577241965\n",
      "21.039806543120502\n",
      "27.839881615813372\n",
      "23.812265024342135\n",
      "14.605159437406869\n",
      "0.23435077705734386\n",
      "35.207054404025534\n",
      "23.362822874333055\n",
      "25.647512064713084\n",
      "24.01492596037513\n",
      "12.104822909364508\n",
      "28.969724500800503\n",
      "17.56054704735714\n",
      "19.167459603938948\n",
      "20.091498322743604\n",
      "8.716008563169417\n",
      "14.397600879678604\n",
      "28.862361564583146\n",
      "23.681420595568486\n",
      "2.593930539297535\n",
      "19.65678027324459\n",
      "17.914803358296695\n",
      "20.947444427080598\n",
      "19.46687970588401\n",
      "19.561430556401262\n",
      "10.251685301949479\n",
      "18.941969388545292\n",
      "28.68753088584485\n",
      "45.65080017957716\n",
      "17.924964862144243\n",
      "32.5229802290692\n",
      "22.71779960198449\n",
      "25.75272773660145\n",
      "20.32756126111244\n",
      "25.47774606861627\n",
      "28.876564873930256\n",
      "12.66609658315376\n",
      "23.704995164436134\n",
      "20.115862568952675\n",
      "41.39805124094681\n",
      "21.079507499372983\n",
      "15.15329860538284\n",
      "22.531272685572127\n",
      "1.5737259263713774\n",
      "17.232648864714186\n",
      "19.235681506990915\n",
      "41.7055598118577\n",
      "17.65631967674077\n",
      "19.65846228008725\n",
      "22.508563995220563\n",
      "19.96648575430602\n",
      "14.125331260073182\n",
      "12.040249700424534\n",
      "35.44681872408499\n",
      "20.706356157882613\n",
      "21.508564899263387\n",
      "15.222457073795912\n",
      "18.161712487342317\n",
      "7.663499926130408\n"
     ]
    }
   ],
   "source": [
    "#Feature Scaling for test data and finding Y_Pred using m_new\n",
    "features_test=[]\n",
    "for i in range(len(X_test[0])):\n",
    "    for j in range(i,len(X_test[0])):\n",
    "        for k in range(j,len(X_test[0])):\n",
    "            features_test.append(X_test[:,i]*X_test[:,j]*X_test[:,k])\n",
    "features_test = np.array(features_test)\n",
    "for i in features_test:\n",
    "    X_test=np.append(X_test,i.reshape(-1,1),axis=1)\n",
    "X_test_new = scaler.transform(X_test)\n",
    "X_test_new=np.append(X_test_new,np.ones(len(X_test_new)).reshape(-1,1),axis=1)\n",
    "Y_Pred=[]\n",
    "for i in X_test_new:\n",
    "    Y_Pred.append(sum(m_new*i))\n",
    "for i in Y_Pred:\n",
    "    print(i)\n",
    "Y_Pred=np.array(Y_Pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb313dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the predicted data\n",
    "np.savetxt(X=Y_Pred,fname='data1.csv',delimiter=\",\",fmt='%.5f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
